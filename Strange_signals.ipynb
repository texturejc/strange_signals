{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282b4f59",
   "metadata": {},
   "source": [
    "# Strange Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c1591",
   "metadata": {},
   "source": [
    "\n",
    "![Image](set.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc2d19-4d57-4e24-b4a5-6aad9818ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import IFrame\n",
    "import random\n",
    "import sympy\n",
    "from collections import Counter\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406384bd",
   "metadata": {},
   "source": [
    "# The problem: How might we decode an alien signal?\n",
    "\n",
    "![The Wow! Signal](wow_signal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547db3b",
   "metadata": {},
   "source": [
    "# The difference between syntax and semantics\n",
    "\n",
    "The first step in decoding any alien language comes with distiguishing beween the rules that allow tokens to be combined, and what these tokens mean.\n",
    "\n",
    "1. Syntax concerns the formal rules that allow words or tokens to be put together in legitimate ways.\n",
    "2. Semantics concerns the meaning of words and word sequences.\n",
    "\n",
    "Take the sentence: \n",
    "\n",
    "## \"April is the cruellest month\". \n",
    "\n",
    "The syntax of this sentence is determined by the gramatical relations between its parts:\n",
    "\n",
    "![Syntax](eliot.png)\n",
    "\n",
    "The semantic structure of the sentence is determined by the meaning of the words and how they map onto the world.\n",
    "\n",
    "![Semantics](april.png)\n",
    "\n",
    "If we are to decode an alien language, we need to analyse both its syntax and its semantics. \n",
    "\n",
    "## How will we procede?\n",
    "\n",
    "### Syntax\n",
    "\n",
    "* We will use a concept called entropy to help us figure out the 'alphabet' of the alien language. This will then allow us to determine its words.\n",
    "\n",
    "### Semantics\n",
    "\n",
    "* We will use AI methods to figure out the semantics of the language, and see if there is any plausible mapping between the ways concepts are combined in human languages and the alien language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766807c",
   "metadata": {},
   "source": [
    "# 1. Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3b226",
   "metadata": {},
   "source": [
    "# Entropy and natural language processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1c9c3",
   "metadata": {},
   "source": [
    "Entropy is a measure of unpredictability; the more unpredictable a system is, the higher its entropy. Though originally formulated in the context of thermodynamics, Claude Shannon extended the concept to information in his landmark work [A Mathematical Theory of Communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf). Because it offers a measure of unpredictability, entropy is of crucual important in NLP, machine learning, and AI. \n",
    "\n",
    "This is the formula for entropy, where $H$ is the entropy measure and $X$ is a discrete probability distribution over $n$ states of a system:\n",
    "\n",
    "$$X = (x_1, x_2, x_3, ... x_n)$$\n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} p(x)\\log_{2} p(x)$$\n",
    "\n",
    "But what does this formula mean? Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ac480",
   "metadata": {},
   "source": [
    "## Surprise\n",
    "\n",
    "The first step in understanding entropy comes with defining the notion of *surprise* (sometimes called *surprisal*, *information*, or *self-information*). When is an event surprising? When it's unlikely but happens anyway. Therefore, surprise is inveresely proportional to probability: high probability events have low surprise (we expect them to occur) while low probability events have high surprise (we *don't* expect them to occur). \n",
    "\n",
    "Let's take an example. What would surprise us most if it fell from the sky?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20688d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['frogs', 'ash', 'snow', 'rain']\n",
    "y = [0.01, 0.1, 0.3, 0.59]\n",
    "\n",
    "sns.barplot(x = x, y = y)\n",
    "plt.ylim(0, 1) \n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('falling items')\n",
    "plt.title(\"What's falling from the sky?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756cedd",
   "metadata": {},
   "source": [
    "### The logarithm function as a measure of surprise\n",
    "\n",
    "How might we represent this mathematically? First, note that every probability must take a value between $0$ and $1$, where the sum of all values in a distribution equals 1. This means that we want a formmula for surprise that turns small probability values into large surprise values, and vice versa. Usefully, the $-log(x)$ function does this between the values of $0$ and $1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb424ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.arange(0,1, 0.0001) # Gets the numbers between 0 and 1 in increments of 0.0001 using the numpy (np) library \n",
    "y = [-np.log2(i) for i in x] # Gets the log base 2 of these numbers\n",
    "\n",
    "sns.lineplot(x = x, y = y) # Plots y against x\n",
    "plt.ylabel('surprise (bits)')\n",
    "plt.xlabel('probability')\n",
    "plt.title('Surprise over probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440ce0c",
   "metadata": {},
   "source": [
    "As can be seen, as probability increases surprise decreases––which is exactly what we want for our measure of surprise. How can we use this to characterise the unpredictability (i.e. the entropy) of an entire system? This is done by multiplying the probability of every state of a system by its surprise, and adding the results together:\n",
    "\n",
    "$$X = (x_1, x_2, ... , x_n)$$\n",
    "\n",
    "$$H(X) = p(x_1)\\times -\\log_{2}p(x_1) + p(x_2)\\times -\\log_{2}p(x_2) + ...+ p(x_n)\\times -\\log_{2}p(x_n)$$\n",
    "\n",
    "Notice that when we represent this using the summation operator ... we get the formula for entropy! \n",
    "\n",
    "$$H(X) = -\\sum_{x \\in X} p(x)\\log_{2} p(x)$$\n",
    "\n",
    "So entropy, in this sense, can be defined as the expected value of surprise across all states or outcomes of a system. That is, it's the 'average' amount of surprise across the system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d46db-774f-4712-883b-d383783e7ad4",
   "metadata": {},
   "source": [
    "## Example 1––A music recommendation system\n",
    "| Title                                    | Genre   | Frequency |\n",
    "|------------------------------------------|---------|-----------|\n",
    "| Blinding Lights - The Weeknd             | Pop     | 25        |\n",
    "| Bad Guy - Billie Eilish                  | Pop     | 40        |\n",
    "| Shake It Off - Taylor Swift              | Pop     | 36        |\n",
    "| Uptown Funk - Mark Ronson ft. Bruno Mars | Pop     | 42        |\n",
    "| Happy - Pharrell Williams                | Pop     | 29        |\n",
    "| Roar - Katy Perry                        | Pop     | 34        |\n",
    "| Sorry - Justin Bieber                    | Pop     | 33        |\n",
    "| Rolling in the Deep - Adele              | Pop     | 27        |\n",
    "| Shape of You - Ed Sheeran                | Pop     | 38        |\n",
    "| Levitating - Dua Lipa                    | Pop     | 30        |\n",
    "| Bohemian Rhapsody - Queen                | Rock    | 78        |\n",
    "| Stairway to Heaven - Led Zeppelin        | Rock    | 65        |\n",
    "| Hotel California - Eagles                | Rock    | 82        |\n",
    "| Back in Black - AC/DC                    | Rock    | 90        |\n",
    "| Sweet Child O' Mine - Guns N' Roses      | Rock    | 74        |\n",
    "| Smells Like Teen Spirit - Nirvana        | Rock    | 68        |\n",
    "| Imagine - John Lennon                    | Rock    | 85        |\n",
    "| Free Fallin' - Tom Petty                 | Rock    | 62        |\n",
    "| Livin' on a Prayer - Bon Jovi            | Rock    | 88        |\n",
    "| Thunderstruck - AC/DC                    | Rock    | 71        |\n",
    "| Jolene - Dolly Parton                    | Country | 18        |\n",
    "| Take Me Home, Country Roads - John Denver| Country | 22        |\n",
    "| The Gambler - Kenny Rogers               | Country | 20        |\n",
    "| Ring of Fire - Johnny Cash               | Country | 25        |\n",
    "| Tennessee Whiskey - Chris Stapleton      | Country | 17        |\n",
    "| Before He Cheats - Carrie Underwood      | Country | 19        |\n",
    "| Take Five - Dave Brubeck                 | Jazz    | 6         |\n",
    "| So What - Miles Davis                    | Jazz    | 8         |\n",
    "| What a Wonderful World - Louis Armstrong | Jazz    | 9         |\n",
    "| Feeling Good - Nina Simone               | Jazz    | 4         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca5ff7-8bff-4f34-902c-87f5b451755f",
   "metadata": {},
   "source": [
    "#### Make our data python compatible by creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0915c31-1d34-4230-8540-d19fa270f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = [\n",
    "    \"Blinding Lights - The Weeknd\",\n",
    "    \"Bad Guy - Billie Eilish\",\n",
    "    \"Shake It Off - Taylor Swift\",\n",
    "    \"Uptown Funk - Mark Ronson ft. Bruno Mars\",\n",
    "    \"Happy - Pharrell Williams\",\n",
    "    \"Roar - Katy Perry\",\n",
    "    \"Sorry - Justin Bieber\",\n",
    "    \"Rolling in the Deep - Adele\",\n",
    "    \"Shape of You - Ed Sheeran\",\n",
    "    \"Levitating - Dua Lipa\",\n",
    "    \"Bohemian Rhapsody - Queen\",\n",
    "    \"Stairway to Heaven - Led Zeppelin\",\n",
    "    \"Hotel California - Eagles\",\n",
    "    \"Back in Black - AC/DC\",\n",
    "    \"Sweet Child O' Mine - Guns N' Roses\",\n",
    "    \"Smells Like Teen Spirit - Nirvana\",\n",
    "    \"Imagine - John Lennon\",\n",
    "    \"Free Fallin' - Tom Petty\",\n",
    "    \"Livin' on a Prayer - Bon Jovi\",\n",
    "    \"Thunderstruck - AC/DC\",\n",
    "    \"Jolene - Dolly Parton\",\n",
    "    \"Take Me Home, Country Roads - John Denver\",\n",
    "    \"The Gambler - Kenny Rogers\",\n",
    "    \"Ring of Fire - Johnny Cash\",\n",
    "    \"Tennessee Whiskey - Chris Stapleton\",\n",
    "    \"Before He Cheats - Carrie Underwood\",\n",
    "    \"Take Five - Dave Brubeck\",\n",
    "    \"So What - Miles Davis\",\n",
    "    \"What a Wonderful World - Louis Armstrong\",\n",
    "    \"Feeling Good - Nina Simone\"\n",
    "]\n",
    "\n",
    "frequencies = [\n",
    "    25, 40, 36, 42, 29, 34, 33, 27, 38, 30,\n",
    "    78, 65, 82, 90, 74, 68, 85, 62, 88, 71,\n",
    "    18, 22, 20, 25, 17, 19,\n",
    "    6, 8, 9, 4\n",
    "]\n",
    "\n",
    "genres = [\n",
    "    \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\", \"Pop\",\n",
    "    \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\", \"Rock\",\n",
    "    \"Country\", \"Country\", \"Country\", \"Country\", \"Country\", \"Country\",\n",
    "    \"Jazz\", \"Jazz\", \"Jazz\", \"Jazz\"\n",
    "]\n",
    "\n",
    "playlist = pd.DataFrame()\n",
    "playlist['song'] = songs\n",
    "playlist['play_count'] = frequencies\n",
    "playlist['genre'] = genres\n",
    "playlist['probability'] = [i/playlist['play_count'].sum() for i in playlist['play_count']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345540b9-572e-4c95-b54e-98531fb49c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_prob = playlist['probability']\n",
    "genres_prob = playlist.groupby('genre').sum()['probability']\n",
    "\n",
    "songs = sp.stats.entropy(songs_prob, base = 2) # Gets the entropy using log base 2 \n",
    "genres = sp.stats.entropy(genres_prob, base = 2)\n",
    "\n",
    "print(\"The entropy of the listener's taste based on songs is {} bits.\".format(songs),\"The entropy of the listener's taste based on genre is {} bits.\".format(genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66ec7f3-81c9-4394-aa0a-21084abea4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = playlist.groupby('genre').sum()['probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_frequencies =  {'E': 0.12003601080324099, 'T': 0.09102730819245775, 'A': 0.08122436731019306, \\\n",
    "                       'O': 0.07682304691407423, 'I': 0.0731219365809743, 'N': 0.06952085625687708, \\\n",
    "                           'S': 0.06281884565369612, 'R': 0.06021806541962589, 'H': 0.05921776532959889, \\\n",
    "                               'D': 0.04321296388916676, 'L': 0.03981194358307493, 'U': 0.028808642592777836, \\\n",
    "                                   'C': 0.027108132439731925, 'M': 0.026107832349704915, 'F': 0.02300690207062119, \\\n",
    "                                       'Y': 0.021106331899569872, 'W': 0.02090627188156447, 'G': 0.020306091827548264, \\\n",
    "                                           'P': 0.01820546163849155, 'B': 0.014904471341402423, 'V': 0.011103330999299792,\\\n",
    "                                               'K': 0.006902070621186356, 'X': 0.0017005101530459142, 'Q': 0.0011003300990297092, \\\n",
    "                                                   'J': 0.0010003000900270084, 'Z': 0.0007002100630189059}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(y = [i for i in letter_frequencies.keys()], x = [i for i in letter_frequencies.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_alphabet = sp.stats.entropy([i for i in letter_frequencies.values()], base = 2)\n",
    "                                     \n",
    "random_freq = [1] * 26\n",
    "random_freq = [1/26 for i in random_freq]\n",
    "                                     \n",
    "entropy_rand = sp.stats.entropy(random_freq, base = 2)\n",
    "                                     \n",
    "print(\"The entropy of the English alphabet is {} bits.\".format(entropy_alphabet),\"The entropy of a system of 26 equiprobable symbols is {} bits.\".format(entropy_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b92daf",
   "metadata": {},
   "source": [
    "# Our alien message\n",
    "\n",
    "What's the most basic way a message can be encoded? It's as a series of 0's and 1's. This is because any transmitting device can transmit or not transmit. The transmissions give us the 1's; the gaps between them give us the 0's. \n",
    "\n",
    "So let's imagine we receive the series of 0's and 1's below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3038a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = '011101110110010100000000011000110110111101101101011001010000000001110110011001010111001001111001000000000110001001110010011010010110010101100110011011000111100100000000011101000110111100000000011101000110100001101001011100110000000001110000011011000110000101100011011001010000000001100001011011100110010000000000011101110110000101110100011000110110100000000000011110010110111101110101011100100000000001110011011101000110000101110010000000000110011001110010011011110110110100000000011000010110011001100001011100100000000001110011011011110000000001110100011010000110000101110100000000000111011101100101000000000110110101100001011110010000000001110011011001010110010100000000011110010110111101110101000000000110100101101110000000000111010001101000011001010000000001101100011010010110011101101000011101000000000001101111011001100000000001111001011011110111010101110010000000000110001001100101011010010110111001100111'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bcac1-5a3e-483e-ac8a-3e60058bf4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aabcd5",
   "metadata": {},
   "source": [
    "## Now, let's generate a random string of the same length for comparison purposes. We do this so we can check whether or not the message is random noise or has a structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c72126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = random.choices([0,1], k=len(text)) # Generates a random selection of 0's and 1's up to length of the message text\n",
    "rand = [str(i) for i in rand] # Turns the selection into string data\n",
    "rand = \"\".join(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264f6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_d = [text.count('0')/len(text), text.count('1')/len(text)] # Gets the probability of selecting a 0 or a 1 in the distribution\n",
    "rand_d = [rand.count('0')/len(rand), rand.count('1')/len(rand)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd64cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_t = sp.stats.entropy(text_d, base = 2)\n",
    "ent_r = sp.stats.entropy(rand_d, base = 2)\n",
    "\n",
    "messages = pd.DataFrame()\n",
    "messages['type'] = ['message', 'random']\n",
    "messages['entropy'] = [ent_t, ent_r]\n",
    "\n",
    "sns.pointplot(x = 'type', y = 'entropy', data = messages)\n",
    "\n",
    "#print(ent_t, ent_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa973ed",
   "metadata": {},
   "source": [
    "## Identifying the alien alphabet\n",
    "\n",
    "This tells is that the message is more predictable than a random string of 0's and 1's, but not much else. Let's make a few assumptions and see how far we can go.\n",
    "\n",
    "* The aliens have a finite number of symbols that are analogous to the letters of the alphabet (i.e. they won't encode words directly)\n",
    "* Each symbol will be encoded by groups of 0's and 1's\n",
    "* The message will try to balance ease of decoding against expressive power\n",
    "* The message is complete\n",
    "* There will be repeated symbols in the message\n",
    "* There will be a distinct (and obvious) character for a space between words\n",
    "\n",
    "The simplest possible symbolic system has only two states:\n",
    "\n",
    "$$(0) = S_1$$\n",
    "$$(1) = S_2$$\n",
    "\n",
    "Now, let's imagine a symbolic system where every symbol is associated with two binary digits. This gives us four possible symbols:\n",
    "\n",
    "$$(0,0) = S_1$$\n",
    "$$(0,1) = S_2$$\n",
    "$$(1,0) = S_3$$\n",
    "$$(1,1) = S_4$$\n",
    "\n",
    "Notice that though the length of the encoding is always the same (i.e. 2), we can get four symbols. \n",
    "\n",
    "But are four symbols enough for an expressive alphabet? Not really. If we encode using three binary digits, we get eight ($2^3$) symbols, which is still too small. But if we use (say) ten digits, we get 1,024 ($2^{10}$) symbols which is likely too big. What's the most likely choice for the aliens to make? One clue comes from the length of the message. If it's complete, then the length of the encoding *must* evenly divide the message length. Our message is 288 digits long.\n",
    "\n",
    "This gives the following options for the length of the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eedac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_length_t = sympy.divisors(len(text)) # Gets the numbers that evenly divide the message length using the `sympy` library for algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_length_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19a3f5",
   "metadata": {},
   "source": [
    "Now that we have all the possible encoding lengths, we can divide up the message into chunks matching each possible 'alphabet'. *One* of these will be the correct one––but we don't know which!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc291d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets_t = []\n",
    "\n",
    "for j in symbol_length_t:\n",
    "    alphabets_t.append([text[i:i+j] for i in range(0, len(text), j)]) # Chunks the message into groups of 0's and 1's corresponding to all possible encoding lengths\n",
    "    \n",
    "\n",
    "# Get a count of how often each possible symbol occurs in the message for each alphabet and convert to pandas series\n",
    "alphabets_t =[pd.Series(Counter(i)) for i in alphabets_t]\n",
    "\n",
    "#sns.lineplot(x = alphabets_t.index, y = alphabets_t.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3181e41a",
   "metadata": {},
   "source": [
    "### So, we need to compare our possible message to our random string so we can see where differences emerge. We therefore do the same chunking for our random string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_length_r = sympy.divisors(len(rand))\n",
    "\n",
    "alphabets_r = []\n",
    "\n",
    "for j in symbol_length_r:\n",
    "    alphabets_r.append([rand[i:i+j] for i in range(0, len(rand), j)])\n",
    "    \n",
    "alphabets_r = [pd.Series(Counter(i)) for i in alphabets_r]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e17bd7",
   "metadata": {},
   "source": [
    "Now, let's calculate the entropy of each possible alphabet for the message string and the random string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22121ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies_t = [sp.stats.entropy(i, base = 2) for i in alphabets_t] # entropies of all message alphabets\n",
    "entropies_r = [sp.stats.entropy(i, base = 2) for i in alphabets_r] # entropies of all random string alphabets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74501bb7",
   "metadata": {},
   "source": [
    "The next step comes with plotting all possible encoding lengths for the both the message and the random string and comparing one against the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a panddas dataframe from the entropy data to compare the message and the random texts\n",
    "\n",
    "entropy = pd.DataFrame()\n",
    "entropy['random'] = entropies_r\n",
    "entropy['message'] = entropies_t\n",
    "\n",
    "entropy_m = pd.melt(entropy)\n",
    "\n",
    "entropy_m.columns = ['source', 'entropy']\n",
    "\n",
    "entropy_m['encoding_length'] = symbol_length_r + symbol_length_t\n",
    "\n",
    "# Plot the entropy for both message texts\n",
    "sns.pointplot(x = 'encoding_length', y = 'entropy', hue = 'source', data = entropy_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049adc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy['difference'] = entropy['random'] - entropy['message']\n",
    "entropy['encoding_length'] = symbol_length_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5269f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab20c6",
   "metadata": {},
   "source": [
    "### What does this tell us? It tells us that each letter of our alien alphabet is encoded with 8 binary digits. But what about words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56821344",
   "metadata": {},
   "source": [
    "You'll remember that one our assumptions was that there would be an obvious candidate for the space between words. What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc9762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_letters = [text[i:i+8] for i in range(0, len(text), 8)]\n",
    "alien_letters_set = list(set(alien_letters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdff3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_letters_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_words = text.split('00000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b456518",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257d52f",
   "metadata": {},
   "source": [
    "# 2. Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b389c",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "\n",
    "The last ten years have seen the study of semantics revolutionised by the use of word embeddings. These are work by locating a word in a high-dimensional space, where words with similar meanings are close to each other in this space. They are derived by training a neural network on large samples of linguistic data. It's task is to learn a mathematical representation that minmimises the distance between words that frequently occur together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = ['hammer', 'chisel', 'axe', 'file', 'cat', 'dog', 'zebra', 'dinosaur', 'apple', 'pear', 'grape', 'melon', \\\n",
    "           'cloud', 'moon', 'sun', 'star', 'ocean', 'france', 'germany', 'australia', 'doctor', 'mechanic', 'vet', 'teacher']\n",
    "\n",
    "concept_v = [nlp(i).vector for i in concepts]\n",
    "\n",
    "vectors_concepts = pd.DataFrame(concept_v, index = [i for i in concepts])\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "comps = pca.fit_transform(vectors_concepts)\n",
    "pc_df = pd.DataFrame(data = comps, columns = ['PC'+str(i) for i in range(1, comps.shape[1]+1)])\n",
    "pc_df['words'] = vectors_concepts.index\n",
    "\n",
    "fig = px.scatter_3d(pc_df, x='PC1', y='PC2', z='PC3', \n",
    "              hover_data = ['words'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0a828",
   "metadata": {},
   "source": [
    "## What's our Rosetta stone?\n",
    "\n",
    "![Rosetta](rosetta.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a949c3-ad5e-45d0-a716-9b470d2c654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alien_vectors = pd.read_pickle(\"alien_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a0d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [\n",
    "    \"atmosphere\",\n",
    "    \"lights\",\n",
    "    \"aurora\",\n",
    "    \"existence\",\n",
    "    \"alive\",\n",
    "    \"thinks\",\n",
    "    \"peace\",\n",
    "    \"exchange\",\n",
    "    \"celestial\",\n",
    "    \"comet\",\n",
    "    \"watching\",\n",
    "    \"constellation\",\n",
    "    \"cosmic\",\n",
    "    \"away\",\n",
    "    \"distance\",\n",
    "    \"crater\",\n",
    "    \"eclipse\",\n",
    "    \"galaxy\",\n",
    "    \"gravity\",\n",
    "    \"interstellar\",\n",
    "    \"meteor\",\n",
    "    \"nebula\",\n",
    "    \"orbit\",\n",
    "    \"planet\",\n",
    "    \"satellite\",\n",
    "    \"solar\",\n",
    "    \"space\",\n",
    "    \"spacecraft\",\n",
    "    \"stars\",\n",
    "    \"skies\",\n",
    "    \"telescope\",\n",
    "    \"universe\",\n",
    "    \"vacuum\",\n",
    "    \"velocity\",\n",
    "    \"zenith\",\n",
    "    \"person\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba302be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [nlp(i).vector for i in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb83146",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_human = pd.DataFrame(vectors, index = [i for i in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f132b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_human['origin'] = 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94373929",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vectors = pd.concat([vectors_human, vectors_alien])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(all_vectors.drop(['origin'], axis = 1))\n",
    "pc_df_1 = pd.DataFrame(data = comps_1, columns = ['PC'+str(i) for i in range(1, comps_1.shape[1]+1)])\n",
    "pc_df_1['words'] = all_vectors.index\n",
    "pc_df_1['origin'] = [i for i in all_vectors['origin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e6e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pc_df_1, x='PC1', y='PC2', z='PC3', color = 'origin',\n",
    "              hover_data = ['words'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e7af7",
   "metadata": {},
   "source": [
    "# Notebook ends"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
